

<!DOCTYPE html>
<html lang="en">
  <head>
    <meta charset="utf-8">
    <title>Statistical Models & Computing Methods, Fall 2020</title>
    
    <meta name="author" content="Jianqing Jia">

    <!-- Enable responsive viewport -->
    <meta name="viewport" content="width=device-width, initial-scale=1.0">

    <!-- Le HTML5 shim, for IE6-8 support of HTML elements -->
    <!--[if lt IE 9]>
      <script src="http://html5shim.googlecode.com/svn/trunk/html5.js"></script>
    <![endif]-->

    <!-- Le styles -->
    <link href="/assets/themes/twitter/bootstrap/css/bootstrap.2.2.2.min.css" rel="stylesheet">
	<!-- <link href="/assets/themes/twitter/css/1.4.0/bootstrap.css" rel="stylesheet"> -->
    <link href="/assets/themes/twitter/css/style.css?body=1" rel="stylesheet" type="text/css" media="all">
    <!-- <link href="/assets/themes/twitter/css/kbroman.css" rel="stylesheet" type="text/css" media="all"> -->

    <!-- Le fav and touch icons -->

    <!-- atom & rss feed -->
    <link href="nil" type="application/atom+xml" rel="alternate" title="Sitewide ATOM Feed">
    <link href="nil" type="application/rss+xml" rel="alternate" title="Sitewide RSS Feed">

  </head>

  <body>
	<nav class="navbar navbar-inverse navbar-static-top" role="navigation">
	    <div class="navbar">
	      <div class="navbar-inner">
	        <div class="container-narrow">
	          <a class="brand" href="/">Jianqing Jia</a>
			  <ul class="nav">
				  <li><a href="/pages/publications.html">Publications</a></li>
				  <li><a href="/pages/courses.html">Teaching</a></li>
				  <li><a href="/pages/talks.html">Talks</a></li>
				  <li><a href="/pages/404.html">Blog</a></li>
				  <!-- <li><a href="/blog">Blog</a></li> -->
				  <li><a href="/static/CV_Cheng_Zhang.pdf">CV</a></li>
			  </ul>
	        </div>
	      </div>
	    </div>
	</nav>

    <div class="container-narrow">

      <div class="content">
        

<div class="page-header">
  <h2>Statistical Models & Computing Methods, Fall 2020 </h2>
</div>

<div class="row-fluid">
  <div class="span12">
    <h3 id="basic-info">Basic Info</h3>
<ul>
  <li>Instructor: Cheng Zhang (<a href="mailto:chengzhang@math.pku.edu.cn">chengzhang@math.pku.edu.cn</a>)</li>
  <li>Class times: Thursday 6:40-9:30pm, Classroom Building No.2, Room 401</li>
  <li>Office hours: Thursday 3:00-5:00pm or by appointment, 1279 Science Building No.1</li>
  <li><a href="/courses/Syllabus-smcm-f20.pdf">Syllabus</a></li>
</ul>

<h3 id="description-and-objectives">Description and Objectives</h3>
<p>Computational statistics is a branch of mathematical sciences focusing on efficient numerical methods for statistical problems. The goal of this course is to provide students an introduction to a variety of modern statistical models and related computing methods. Topics include numerical optimization in statistical inference including expectation-maximization (EM) algorithm, Fisher scoring, gradient descent and stochastic gradient descent, etc., numerical integration approaches include basic numerical quadrature and Monte Carlo methods, and approximate Bayesian inference methods including Markov chain Monte Carlo, variational inference and their scalable counterparts, with applications in statistical machine learning, computational biology and other related fields. Additional topics may vary. Coursework will include computer assignments.</p>

<h3 id="prerequisites">Prerequisites</h3>
<p>Multivariate calculus, linear algebra, graduate level courses in probability and statistics, applied stochastic processes</p>

<h3 id="recommended-textbooks">Recommended Textbooks</h3>
<ul>
  <li>Givens, G. H. and Hoeting, J. A. (2005) Computational Statistics, 2nd Edition, Wiley-Interscience.</li>
  <li>Gelman, A., Carlin, J., Stern, H., and Rubin, D. (2003). Bayesian Data Analysis, 2nd Edition, Chapman &amp; Hall.</li>
  <li>Liu, J. (2001). Monte Carlo Strategies in Scientific Computing, Springer-Verlag.</li>
  <li>Lange, K. (2002). Numerical Analysis for Statisticians, Springer-Verlag, 2nd Edition.</li>
  <li>Hastie, T., Tibshirani, R. and Friedman, J. (2009). The Elements of Statistical Learning, 2nd Edition, Springer.</li>
  <li>Goodfellow, I., Bengio, Y. and Courville, A. (2016). Deep Learning, MIT Press.</li>
</ul>

<h3 id="grading">Grading</h3>
<ul>
  <li>Homework (60%): 4 problem sets (15% each)</li>
  <li>final project (40%): midterm proposal (5%) + oral presentation (10%) + final write-up (25%)</li>
</ul>

<p>There will be <code class="language-plaintext highlighter-rouge">7</code> free late days in total, use them in your own ways. Afterwards, late homework will be discounted by 25% for each additional day. Not acceptable after <code class="language-plaintext highlighter-rouge">3</code> late days per problem set (PS). Late policy <code class="language-plaintext highlighter-rouge">does not apply</code> to the final project, please submit it on time. Discussing assignments verbally with your classmates is allowed and encouraged. However, you should finish your work independently. Identified cheating incidents will be reported and will result in zero grades.</p>

<h3 id="computer-and-technical-requirements">Computer and Technical Requirements</h3>

<p>We will use python during the course. A good Python tutorial is available at <a href="http://www.scipy-lectures.org/">http://www.scipy-lectures.org/</a>. You may also find another shorter tutorial useful at <a href="http://cs231n.github.io/python-numpy-tutorial/">http://cs231n.github.io/python-numpy-tutorial/</a>. If you have never used Python before, I recommend using Anaconda Python 3.7 <a href="https://www.continuum.io/">https://www.continuum.io/</a>.</p>

<h3 id="lectures">Lectures</h3>
<ul>
  <li>09/24/2020: <a href="/static/slides/smcm_fall20/lec01.pdf">Lecture 1 - Introduction</a><br />
Textbook on convex optimization: <a href="https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf">https://web.stanford.edu/~boyd/cvxbook/bv_cvxbook.pdf</a></li>
  <li>10/15/2020: <a href="/static/slides/smcm_fall20/lec02.pdf">Lecture 2 - Gradient Methods</a></li>
  <li>10/22/2020: <a href="/static/slides/smcm_fall20/lec03.pdf">Lecture 3 - Numerical Integration</a></li>
  <li>10/29/2020: <a href="/static/slides/smcm_fall20/lec04.pdf">Lecture 4 - Markov Chain Monte Carlo</a><br />
Handbook of Markov Chain Monte Carlo: <a href="https://www.mcmchandbook.net">https://www.mcmchandbook.net</a></li>
  <li>11/05/2020: <a href="/static/slides/smcm_fall20/lec05.pdf">Lecture 5 - Advanced MCMC</a></li>
  <li>11/12/2020: <a href="/static/slides/smcm_fall20/lec06.pdf">Lecture 6 - Scalable MCMC</a></li>
  <li>11/19/2020: <a href="/static/slides/smcm_fall20/lec07.pdf">Lecture 7 - Expectation Maximization</a></li>
  <li>11/26/2020: <a href="/static/slides/smcm_fall20/lec08.pdf">Lecture 8 - Variational Inference</a></li>
  <li>12/03/2020: <a href="/static/slides/smcm_fall20/lec09.pdf">Lecture 9 - Stochastic Variational Inferece and Alternative Training Objectives</a></li>
  <li>12/10/2020: <a href="/static/slides/smcm_fall20/lec10.pdf">Lecture 10 - Advanced VI</a></li>
  <li>12/17/2020: <a href="/static/slides/smcm_fall20/lec11.pdf">Lecture 11 - Autoregressive Models and Variational Autoencoders</a></li>
  <li>12/24/2020: <a href="/static/slides/smcm_fall20/lec12.pdf">Lecture 12 - Generative Adversarial Nets and Bayesian Phylogenetic Inference</a></li>
</ul>

<h3 id="assignments">Assignments</h3>
<ul>
  <li>10/15/2020: <a href="/static/slides/smcm_fall20/hw01.pdf">Homework 1</a>, <strong>Due</strong> <code class="language-plaintext highlighter-rouge">10/29/2020</code></li>
  <li>11/05/2020: <a href="/static/slides/smcm_fall20/hw02.pdf">Homework 2</a>, <strong>Due</strong> <code class="language-plaintext highlighter-rouge">11/19/2020</code>   Data: <a href="/static/datasets/mcs_hw2_p3_data.npy">p3</a></li>
  <li>11/25/2020: <a href="/static/slides/smcm_fall20/hw03.pdf">Homework 3</a>, <strong>Due</strong> <code class="language-plaintext highlighter-rouge">12/10/2020</code>   Data: <a href="/static/datasets/mog_data.npy">p2</a>, <a href="/static/datasets/proteins.zip">p3</a></li>
  <li>12/17/2020: <a href="/static/slides/smcm_fall20/hw04.pdf">Homework 4</a>, <strong>Due</strong> <code class="language-plaintext highlighter-rouge">01/07/2021</code>   Data: <a href="/static/datasets/mcs_hw4_p1_lda.npy">p1</a>, <a href="/static/datasets/banana_shape_data.npy">p3</a></li>
</ul>

<h3 id="final-project">Final Project</h3>
<p>You may structure your project exploration around a general problem type, algorithm, or data set, but should explore around your problem, testing thoroughly or comparing to alternatives. You may work on the project as teams. Each team may have up to <code class="language-plaintext highlighter-rouge">4</code> people. Please form your team by the end the <code class="language-plaintext highlighter-rouge">4th</code> week. You should present a project proposal that briefly describe your project concept and goals in one slide on <code class="language-plaintext highlighter-rouge">11/12</code>. You should turn in a write-up (&lt; 10 pages) describing your project and its outcomes, similar to a research-level publication. I suggest the latex styles for <a href="https://nips.cc/Conferences/2019/PaperInformation/StyleFiles">NeurIPS</a> or <a href="https://iclr.cc/Conferences/2019/CallForPapers">ICLR</a>. There will be in class project presentation at the end of the term. Not presenting your projects will be taken as voluntarily <code class="language-plaintext highlighter-rouge">giving up</code> the opportunity for the final write-ups.</p>

<h3 id="tentative-schedule">Tentative Schedule</h3>

<table>
  <thead>
    <tr>
      <th>Week</th>
      <th>Date</th>
      <th>Topics</th>
      <th>Notes</th>
    </tr>
  </thead>
  <tbody>
    <tr>
      <td>1</td>
      <td>09/24</td>
      <td>Introduction, Convex Optimization</td>
      <td> </td>
    </tr>
    <tr>
      <td>2</td>
      <td>10/01</td>
      <td>–</td>
      <td>National Day</td>
    </tr>
    <tr>
      <td>3</td>
      <td>10/08</td>
      <td>–</td>
      <td>National Day</td>
    </tr>
    <tr>
      <td>4</td>
      <td>10/15</td>
      <td>Iterative Reweighted Least Squares, Gradient Descent Methods</td>
      <td> </td>
    </tr>
    <tr>
      <td>5</td>
      <td>10/22</td>
      <td>Numerical Quadrature, Monte Carlo Methods, Variance Reduction Techniques</td>
      <td> </td>
    </tr>
    <tr>
      <td>6</td>
      <td>10/29</td>
      <td>Markov Chain Monte Carlo, Improving Mixing and Convergence</td>
      <td> </td>
    </tr>
    <tr>
      <td>7</td>
      <td>11/05</td>
      <td>Auxiliary Variable Methods, Hamiltonian Monte Carlo, Adaptive MCMC</td>
      <td> </td>
    </tr>
    <tr>
      <td>8</td>
      <td>11/12</td>
      <td>Scalable MCMC Methods</td>
      <td><code class="language-plaintext highlighter-rouge">Proposal Presentation</code></td>
    </tr>
    <tr>
      <td>9</td>
      <td>11/19</td>
      <td>Expectation Maximization, Convergence Theory and EM Variants</td>
      <td> </td>
    </tr>
    <tr>
      <td>10</td>
      <td>11/26</td>
      <td>Variational Bayesian EM, Variational Inference, Mean Field VI</td>
      <td> </td>
    </tr>
    <tr>
      <td>11</td>
      <td>12/03</td>
      <td>Stochastic Variational Inference, Choice of Training Objectives</td>
      <td> </td>
    </tr>
    <tr>
      <td>12</td>
      <td>12/10</td>
      <td>Normalizing Flow, Combinig VI and MCMC</td>
      <td> </td>
    </tr>
    <tr>
      <td>13</td>
      <td>12/17</td>
      <td>Autoregressive Models, Variational Autoencoder</td>
      <td> </td>
    </tr>
    <tr>
      <td>14</td>
      <td>12/24</td>
      <td>Generative Adversarial Networks, Bayesian Phylogenetic Inference</td>
      <td> </td>
    </tr>
    <tr>
      <td>15</td>
      <td>12/31</td>
      <td>Project Presentation</td>
      <td> </td>
    </tr>
    <tr>
      <td>16</td>
      <td>01/07</td>
      <td>Project Presentation</td>
      <td> </td>
    </tr>
  </tbody>
</table>

  </div>
</div>


      </div>
      <hr>
      <footer>
        <p><small>
  <!-- start of Karl's footer; modify this part -->
          <!-- <a href="https://creativecommons.org/publicdomain/zero/1.0/"><img src="https://i.creativecommons.org/p/zero/1.0/88x31.png" alt="CC0"/></a> &nbsp;
          <a href="https://kbroman.org">Karl Broman</a> -->
  <!-- end of Karl's footer; modify this part -->
        </small></p>
      </footer>

    </div>

    
  </body>
</html>

